{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMx2CaxbTqqBN6vL6Hxp+TC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHL-edu/backup/blob/main/Fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R9yJk1mZW9ga"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test Launch Fine_turning\n",
        "from transformers import BertTokenizerFast\n",
        "import os\n",
        "# 加载预训练分词器\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
        "\n",
        "# 准备领域词汇（手动或从语料提取）\n",
        "new_tokens = [\"奇怪\",\"乌鸦\"]  # 示例词汇\n",
        "\n",
        "# 添加新词汇\n",
        "tokenizer.add_tokens(new_tokens)\n",
        "\n",
        "# 保存分词器\n",
        "SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer'\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "tokenizer.save_pretrained(SAVE_PATH)"
      ],
      "metadata": {
        "id": "vfChp1u7XbHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test above Fine_turning learing what is deferent\n",
        "from transformers import BertTokenizerFast\n",
        "import os\n",
        "# 加载预训练分词器\n",
        "SAVE_PATH = '/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer'\n",
        "try:\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(SAVE_PATH)\n",
        "  #tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
        "except:\n",
        "  print(\"tokenizer not found\")\n",
        "\n",
        "text = \"奇怪的乌鸦需要喝水\"\n",
        "tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))"
      ],
      "metadata": {
        "id": "KW_4aJ8qYPV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#处理json保留data_long项\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import json\n",
        "import csv\n",
        "import os\n",
        "data_long = 264430\n",
        "# 路径设置\n",
        "JSON_PATH = '/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/ci.json'\n",
        "CSV_PATH = f'/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/ci_200000_{data_long}.csv'\n",
        "os.makedirs(os.path.dirname(CSV_PATH), exist_ok=True)  # 确保目录存在\n",
        "\n",
        "# 读取JSON数据\n",
        "with open(JSON_PATH, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)  # 假设data是列表格式\n",
        "\n",
        "# 提取前100项的ci字段\n",
        "ci_list = []\n",
        "for item in data[100000:data_long]:\n",
        "    if isinstance(item, dict) and 'ci' in item:\n",
        "        ci_list.append([item['ci']])  # 每行作为列表元素\n",
        "\n",
        "# 写入CSV\n",
        "with open(CSV_PATH, 'w', encoding='utf-8', newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(ci_list)  # 写入数据\n",
        "\n",
        "print(f\"前{data_long}项数据已保存至 {CSV_PATH}\")"
      ],
      "metadata": {
        "id": "k_jnE-YScD9Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#本地路径加载\n",
        "from google.colab import drive\n",
        "from transformers import BertModel\n",
        "import os\n",
        "\n",
        "# 1. 挂载Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. 定义模型路径（请根据实际情况修改）\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Bert-Chinese_base'\n",
        "\n",
        "# 3. 验证路径是否存在\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"模型路径不存在: {model_path}\")\n",
        "\n",
        "# 4. 验证必要的模型文件是否存在\n",
        "required_files = ['config.json', 'pytorch_model.bin', 'vocab.txt']\n",
        "missing_files = [f for f in required_files if not os.path.exists(os.path.join(model_path, f))]\n",
        "\n",
        "if missing_files:\n",
        "    raise FileNotFoundError(f\"缺少必要的模型文件: {missing_files}\")\n",
        "\n",
        "# 5. 打印验证通过信息\n",
        "print(\"验证通过，模型文件完整:\")\n",
        "for f in required_files:\n",
        "    print(f\"- {f} ✔\")\n",
        "\n",
        "# 6. 加载模型\n",
        "try:\n",
        "    print(\"\\n正在加载模型...\")\n",
        "    model = BertModel.from_pretrained(model_path)\n",
        "    print(\"模型加载成功!\")\n",
        "\n",
        "    # 7. 打印模型信息\n",
        "    print(\"\\n模型架构:\")\n",
        "    print(model.config)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n模型加载失败: {str(e)}\")\n",
        "    print(\"\\n可能的原因:\")\n",
        "    print(\"1. 模型文件损坏\")\n",
        "    print(\"2. 文件权限问题\")\n",
        "    print(\"3. 路径中包含特殊字符或空格\")\n",
        "    print(\"4. Transformers版本不兼容\")\n",
        "    print(\"\\n建议解决方案:\")\n",
        "    print(\"1. 重新克隆模型仓库\")\n",
        "    print(\"2. 检查并修复文件权限\")\n",
        "    print(\"3. 尝试简化路径名称\")\n",
        "    print(\"4. 更新transformers库: !pip install --upgrade transformers\")"
      ],
      "metadata": {
        "id": "hnfRWVl2hV4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test above Fine_turning learing what is deferent\n",
        "from transformers import BertTokenizerFast\n",
        "import os\n",
        "# 加载预训练分词器\n",
        "Ci100_PATH = '/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer-ci'\n",
        "try:\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(SAVE_PATH)\n",
        "  #tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
        "except:\n",
        "  print(\"tokenizer not found\")\n",
        "\n",
        "text = \"晨夕晨乌大傻子\"\n",
        "tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))"
      ],
      "metadata": {
        "id": "A_15N7IxRni8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import BertModel, BertTokenizerFast\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. 挂载Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "# 2. 定义参数和路径\n",
        "data_init = 100000\n",
        "data_long = 200000\n",
        "ORIGIN_BERT_PATH = f'/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer_ci_{data_init}'\n",
        "CSV_PATH = f'/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/ci_{data_init}_{data_long}.csv'\n",
        "SAVE_PATH = f'/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer_ci_{data_init}_{data_long}'\n",
        "\n",
        "# 3. 加载预训练分词器和模型\n",
        "try:\n",
        "    if not os.path.exists(ORIGIN_BERT_PATH):\n",
        "        raise FileNotFoundError(f\"BERT model path not found: {ORIGIN_BERT_PATH}\")\n",
        "    tokenizer = BertTokenizerFast.from_pretrained(ORIGIN_BERT_PATH)\n",
        "    model = BertModel.from_pretrained(ORIGIN_BERT_PATH)\n",
        "    print(\"Tokenizer and model loaded successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading tokenizer or model: {str(e)}\")\n",
        "    exit(1)\n",
        "\n",
        "# 4. 加载CSV文件中的领域词汇\n",
        "try:\n",
        "    if not os.path.exists(CSV_PATH):\n",
        "        raise FileNotFoundError(f\"CSV file not found: {CSV_PATH}\")\n",
        "    df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "    # 验证CSV文件是否为空\n",
        "    if df.empty:\n",
        "        raise ValueError(\"CSV file is empty.\")\n",
        "\n",
        "    # 假设词汇在第一列，检查列是否存在\n",
        "    if df.shape[1] < 1:\n",
        "        raise ValueError(\"CSV file has no columns.\")\n",
        "    new_tokens = df.iloc[:, 0].dropna().tolist()  # 提取第一列，去除空值并转为列表\n",
        "    print(f\"Loaded {len(new_tokens)} tokens from CSV.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error processing CSV file: {str(e)}\")\n",
        "    exit(1)\n",
        "\n",
        "# 5. 添加新词汇到分词器并调整模型嵌入层\n",
        "try:\n",
        "    num_added = tokenizer.add_tokens(new_tokens)\n",
        "    print(f\"Added {num_added} new tokens to the tokenizer.\")\n",
        "\n",
        "    # 调整BERT模型的嵌入层以匹配新词汇表\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "    print(\"Model token embeddings resized to match tokenizer.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error adding tokens or resizing model embeddings: {str(e)}\")\n",
        "    exit(1)\n",
        "\n",
        "# 6. 保存分词器和模型\n",
        "try:\n",
        "    os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "    tokenizer.save_pretrained(SAVE_PATH)\n",
        "    model.save_pretrained(SAVE_PATH)\n",
        "    print(f\"Tokenizer and model saved to {SAVE_PATH}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error saving tokenizer or model: {str(e)}\")\n",
        "    exit(1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNDa2LOheQsI",
        "outputId": "f0b224ff-958c-4213-df99-8e885e360771"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test above Fine_turning learing what is deferent\n",
        "from transformers import BertTokenizerFast\n",
        "import os\n",
        "data_long = 264430\n",
        "# 加载预训练分词器\n",
        "Ci100_PATH = f'/content/drive/MyDrive/Colab Notebooks/BERT/Fine_turning/Tokenizer_ci_{data_long}'\n",
        "try:\n",
        "  tokenizer = BertTokenizerFast.from_pretrained(SAVE_PATH)\n",
        "  #tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")\n",
        "except:\n",
        "  print(\"tokenizer not found\")\n",
        "\n",
        "text = \"晨夕是大傻子\"\n",
        "tokens = tokenizer(text, return_tensors=\"pt\")\n",
        "print(tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4HH2uKgDez9X",
        "outputId": "6e3a6e97-20af-4ef3-b3d7-21eca9469e3c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['[CLS]', '晨夕', '是', '大', '傻', '子', '[SEP]']\n"
          ]
        }
      ]
    }
  ]
}