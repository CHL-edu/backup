{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CHL-edu/backup/blob/main/deepseek1_5b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ1wu3BpKXmw",
        "outputId": "602e1a2e-0f50-4a39-cb6e-b22ad07ac491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# 挂载 Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 定义模型保存路径\n",
        "model_save_path = '/content/drive/MyDrive/Colab Notebooks/deepseek1.5b'\n",
        "os.makedirs(model_save_path, exist_ok=True)\n",
        "\n",
        "# 检查 GPU\n",
        "if not torch.cuda.is_available():\n",
        "    raise RuntimeError(\"未检测到 GPU，请启用 GPU 运行时！\")\n",
        "print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "# 安装 bitsandbytes\n",
        "print(\"安装 bitsandbytes...\")\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9gTxmJbdDXJ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from huggingface_hub import snapshot_download\n",
        "import torch\n",
        "\n",
        "# 下载模型\n",
        "model_name = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
        "if not os.path.exists(os.path.join(model_save_path, \"config.json\")):\n",
        "    print(f\"下载模型到 {model_save_path}...\")\n",
        "    snapshot_download(repo_id=model_name, local_dir=model_save_path, local_dir_use_symlinks=False)\n",
        "    print(\"下载完成！\")\n",
        "else:\n",
        "    print(f\"模型已存在于 {model_save_path}。\")\n",
        "\n",
        "# 加载分词器\n",
        "print(\"加载分词器...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_save_path)\n",
        "\n",
        "# 设置 pad_token_id\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(f\"pad_token_id 设置为: {tokenizer.eos_token_id}\")\n",
        "\n",
        "# 配置 8-bit 量化\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# 加载模型（GPU，8-bit 量化，sdpa）\n",
        "print(\"加载模型到 GPU...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_save_path,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    quantization_config=quant_config,\n",
        "    attn_implementation=\"sdpa\"\n",
        ")\n",
        "\n",
        "# 禁用 Sliding Window Attention\n",
        "if hasattr(model.config, \"sliding_window\"):\n",
        "    model.config.sliding_window = None\n",
        "    print(\"已禁用 Sliding Window Attention。\")\n",
        "\n",
        "# 设置模型 pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "print(\"模型加载成功！\")\n",
        "\n",
        "# 交互循环\n",
        "print(\"\\n欢迎使用 DeepSeek-1.5B！输入 '退出' 结束。\")\n",
        "while True:\n",
        "    prompt = input(\"问题：\")\n",
        "    if prompt.strip().lower() == \"退出\":\n",
        "        print(\"退出程序。\")\n",
        "        break\n",
        "\n",
        "    # 编码输入\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "        return_attention_mask=True\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "    # 生成输出\n",
        "    outputs = model.generate(\n",
        "        input_ids=inputs.input_ids,\n",
        "        attention_mask=inputs.attention_mask,\n",
        "        max_length=100,  # 减小长度，避免冗余\n",
        "        temperature=0.6,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        repetition_penalty=1.2,  # 惩罚重复内容\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    # 解码输出\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(\"\\n回答：\")\n",
        "    print(response)\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "# 清理显存\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "print(\"已清理显存。\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM8RAyiwHvr2ZJdqvbjcTU2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}